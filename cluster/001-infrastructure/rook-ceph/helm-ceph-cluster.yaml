---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  maxHistory: 1
  interval: 5m
  timeout: 30m
  install:
    remediation:
      retries: 3
  chart:
    spec:
      # renovate: registryUrl=https://charts.rook.io/release
      chart: rook-ceph-cluster
      version: v1.19.1
      sourceRef:
        kind: HelmRepository
        name: rook
        namespace: flux-system
  values:
    # 设置ceph.conf的覆盖内容，会被通过rook-config-override ConfigMap提供给所有Ceph容器
    # 根据Rook的文档描述，应优先使用Ceph的命令行或仪表盘进行设置
    # 因为通过配置文件设置的方式并不灵活（需要手动重启容器），且可能会出现配置文件和命令行/仪表盘设置不一样的情况，导致管理混乱
    # https://rook.io/docs/rook/v1.12/Storage-Configuration/Advanced/ceph-configuration/?h=ceph.conf#custom-cephconf-settings
    # 但这里依然进行设置的原因是方便统一管理，给其他集群复制配置部署时也可以作为默认值，避免忘记设置

    # 由于Ceph的程序似乎未正确使用UTF-8解码，如果出现中文注释会导致OSD无法启动，报出UnicodeDecodeError，无法直接将中文注释写在configOverride中
    # 所以单独建了一个带注释的ceph.conf文件，用于存放带中文注释的版本
    configOverride: |
      [global]
      bdev_enable_discard = true
      bdev_async_discard = true
      mon_osd_full_ratio = 0.98
      mon_osd_backfillfull_ratio = 0.96
      mon_osd_nearfull_ratio = 0.94
      osd_scrub_max_interval = 2592000
      osd_deep_scrub_interval = 7776000
      osd_pool_default_pg_autoscale_mode = off
      mon_pg_warn_max_object_skew = 20
      rbd_read_from_replica_policy = balance
      
      [mon]
      mon_data_avail_warn = 10
      
      [osd]
      osd_scrub_load_threshold = 10
      osd_max_scrubs = 1
      osd_scrub_during_recovery = false
      osd_max_backfills = 3
      osd_recovery_op_priority = 3
      osd_backfill_retry_interval = 0
      osd_recovery_sleep = 0
      osd_recovery_max_active = 512
      osd_recovery_max_single_start = 512

    # Rook的Ceph调试工具箱容器，可以在容器内执行Ceph命令，方便调试
    toolbox:
      # 开启工具箱容器的部署，默认未开启
      enabled: true

    # Prometheus集成，可以上报数据到Prometheus用于监控或报警
    monitoring:
      # 开启Prometheus集成，默认未开启
      enabled: true
      # 开启自动创建Ceph警告相关的Prometheus规则，默认未开启
      createPrometheusRules: true

    cephClusterSpec:
      # 指定Ceph版本
      cephVersion: 
        image: quay.io/ceph/ceph:v20.2.0

      # 是否在PG状态非完全健康的情况下也允许升级容器版本，避免刚好在升级时触发Scrub、Backfill、Recovery等操作导致PG状态不健康而无法升级
      continueUpgradeAfterChecksEvenIfNotHealthy: true

      mon:
        # 是否允许在同一个节点上运行多个MON
        allowMultiplePerNode: false

      mgr:
        # 是否允许在同一个节点上运行多个MGR
        allowMultiplePerNode: false
        # 手动管理的MGR模块
        modules:
          # Rook集成模块，默认未开启，开启后可以在命令行和仪表盘中，通过Rook操作K8S管理Ceph的各组件运行状态
          - name: rook
            enabled: true

      # Ceph的仪表盘，可以在Web上管理Ceph集群和监控集群状态
      dashboard:
        # 显示开启Ceph Dashboard，默认就已开启
        enabled: true
        # 关闭Ceph Dashboard的SSL，锁定为HTTP模式，避免因为Ceph自己生成的自签名证书导致无法正常通过Ingress访问
        ssl: false
        # 设置Prometheus的访问URL，使Ceph的Dashboard可以正常显示监控内容
        prometheusEndpoint: http://kube-prometheus-stack-prometheus.monitoring-system:9090

      # Ceph组件的K8S部署策略，设置以添加额外的策略
      placement:
        # 对所有Ceph组件生效
        all:
          # 设置必须在有ceph角色的K8S节点上运行Ceph核心组件
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/ceph
                      operator: Exists

      # Ceph组件的K8S资源请求和限制量，设置以覆盖Rook的默认设置
      resources:
        mgr:
          limits:
            # MGR有时会因为高负载而需要大量内存，Rook默认的内存限制过低，调高以确保稳定，避免反复杀进程导致异常
            memory: "16Gi"
          requests:
            # 调低CPU请求量，避免资源浪费
            cpu: "100m"
        mon:
          requests:
            # 调低CPU请求量，避免资源浪费
            cpu: "100m"
        osd:
          limits:
            # OSD在进行Recovery操作时可以吃掉多达8G左右的内存，Rook默认的内存限制过低，调高以确保稳定，避免反复杀进程导致异常
            # 在进行Recovery操作时，可能出现OSD刚启动就吃掉大量内存的情况，如果内存限制过低甚至连OSD启动时的加载PG操作都还没完成（OSD状态为down，还未变为up）就被杀掉了
            memory: "10Gi"
          requests:
            # 调低CPU请求量，避免资源浪费
            cpu: "100m"
            # OSD在日常状态下需要的内存为1G左右，Rook默认的内存请求过高，调低以避免占用过多K8S中节点被标记的内存请求量
            # 在节点内存冗余量小，且OSD较多的情况下，需要确保留有OSD内存膨胀的空间
            memory: "1Gi"

      # 当OSD被标记为out且安全可移除时，允许Rook自动清理
      removeOSDsIfOutAndSafeToRemove: false

      # 设置需要添加到Ceph集群中的硬盘，以及OSD的初始化设置
      storage:
        # 关闭自动在所有节点和所有磁盘上寻找空白盘并添加为OSD的操作，改为手动配置方式添加
        useAllNodes: false
        useAllDevices: false
        nodes:
          - name: "home-server-power"
            devices:
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_9AHBDGTR"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_9BGN94WE"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_9BGTUAXH"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_9BGTV7GH"
                config:
                  deviceClass: "hdd"
          - name: "home-server-titan"
            devices:
              - name: "/dev/disk/by-id/scsi-SWDC_WUH722020BL5204_8LHPJ7WN"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SWDC_WUH722020BL5204_8LHPZ97R"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SWDC_WUH722020BL5204_8LHPZ1KR"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_8LJ6WYZN"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SWDC_WUH721818AL5204_5DKPUPBR"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_8LJS5Z9K"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_9AHBDHBR"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_9AHBDJ9R"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_WDC_WUH722020BL_9AHBRAHR"
                config:
                  deviceClass: "hdd"
          - name: "home-server-nut"
            devices:
              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD71JSA"
                config:
                  deviceClass: "hdd"
                  metadataDevice: "/dev/ceph-osd-db/osd-db-scsi-SATA_ST6000NM0115-1YZ_ZAD71JSA"
                  databaseSizeMB: "30720"  # 30GB
              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD41FWR"
                config:
                  deviceClass: "hdd"
                  metadataDevice: "/dev/ceph-osd-db/osd-db-scsi-SATA_ST6000NM0115-1YZ_ZAD41FWR"
                  databaseSizeMB: "30720"  # 30GB
              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD6XBFD"
                config:
                  deviceClass: "hdd"
                  metadataDevice: "/dev/ceph-osd-db/osd-db-scsi-SATA_ST6000NM0115-1YZ_ZAD6XBFD"
                  databaseSizeMB: "30720"  # 30GB
              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0JMTX"
                config:
                  deviceClass: "hdd"
                  metadataDevice: "/dev/ceph-osd-db/osd-db-scsi-SATA_ST8000NM000A-2KE_WRD0JMTX"
                  databaseSizeMB: "30720"  # 30GB
              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0NB7Y"
                config:
                  deviceClass: "hdd"
                  metadataDevice: "/dev/ceph-osd-db/osd-db-scsi-SATA_ST8000NM000A-2KE_WRD0NB7Y"
                  databaseSizeMB: "30720"  # 30GB
              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0NC8L"
                config:
                  deviceClass: "hdd"
                  metadataDevice: "/dev/ceph-osd-db/osd-db-scsi-SATA_ST8000NM000A-2KE_WRD0NC8L"
                  databaseSizeMB: "30720"  # 30GB
          - name: "home-server-sonic"
            devices:
              - name: "/dev/disk/by-id/nvme-INTEL_SSDPE2KX040T8_BTLJ848209YE4P0DGN"  # Intel P4510 4T U.2
                config:
                  deviceClass: "nvme"
                  osdsPerDevice: "4"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS且降低读写延迟（https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments）
              - name: "/dev/disk/by-id/nvme-INTEL_SSDPE2KX040T8_PHLJ8293003U4P0DGN"  # Intel P4510 4T U.2
                config:
                  deviceClass: "nvme"
                  osdsPerDevice: "4"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS且降低读写延迟（https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments）
              - name: "/dev/disk/by-id/nvme-INTEL_SSDPF2KX076T1_BTAX253201AQ7P6DGN"  # Intel P5520 7.68T U.2
                config:
                  deviceClass: "nvme"
                  osdsPerDevice: "4"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS且降低读写延迟（https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments）
              - name: "/dev/disk/by-id/nvme-INTEL_SSDPF2KX076T1_BTAX2532023H7P6DGN"  # Intel P5520 7.68T U.2
                config:
                  deviceClass: "nvme"
                  osdsPerDevice: "4"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS且降低读写延迟（https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments）

      # Ceph组件的K8S健康监测设置，设置以覆盖Rook的默认设置
      healthCheck:
        livenessProbe:
          osd:
            probe:
              # OSD在硬盘容量较大且已写入较多数据的情况下，启动时加载PG的时间可能会较长，需要设置足够长的初始等待时间，以避免部分OSD因启动时间过长而被反复杀进程导致异常
              initialDelaySeconds: 240

    # Ceph仪表盘的K8S Ingress设置
    ingress:
      dashboard:
        ingressClassName: "traefik"
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: "kube-system-rfc1918@kubernetescrd"
        host:
          name: "ceph.${SECRET_DOMAIN}"
        tls:
          - secretName: "${SECRET_DOMAIN/./-}-tls"

    # Ceph的块存储存储池和对应的K8S StorageClass设置
    cephBlockPools:
      # 覆盖自动创建的.mgr存储池配置
      - name: builtin-mgr  # 点号开头的名称对于K8S来说不是规范名称，使用其他名称
        spec:
          name: .mgr  # 指定存储池名称，不填则按照CephBlockPool的名称自动生成，指定.mgr以覆盖默认的配置
          failureDomain: osd  # 设置存储池的故障域
          replicated: # 设置为副本池
            size: 3  # 副本数
          deviceClass: nvme  # 设置后端osd类型
          parameters:
            compression_mode: aggressive # 设置除了指定不可压缩的数据以外都进行压缩
        # Ceph内部使用的存储池，不需要StorageClass
        storageClass:
          enabled: false  # 是否启用对应的K8S StorageClass，可以仅创建Ceph存储池但不创建StorageClass

      # 基于JuiceFS提供服务的常规应用数据存储池，开启压缩，适用于没有大量随机写的场景（如配置文件、日志、静态资源文件）
      # ⚠️不适用于有大量随机写（会使JuiceFS产生大量碎片）的应用，碎片会导致JuiceFS占用巨量空间，尤其是数据库持续高负载时极其容易将存储撑爆
      - name: juicefs-application-compressed
        spec:
          failureDomain: osd  # 设置存储池的故障域
          replicated: # 设置为副本池
            size: 3  # 副本数
          deviceClass: nvme  # 设置后端osd类型
          parameters:
            compression_mode: aggressive # 设置除了指定不可压缩的数据以外都进行压缩
            pg_num: "512" # 设置初始的PG数量
            pgp_num: "512" # 设置初始的PGP数量，需要和pg_num一致
        # 由于是JuiceFS使用的存储池，所以不需要创建Ceph CSI提供的K8S StorageClass，而是使用JuiceFS CSI提供的K8S StorageClass
        storageClass:
          enabled: false  # 是否启用对应的K8S StorageClass，可以仅创建Ceph存储池但不创建StorageClass

      # 基于Ceph RBD提供服务的常规应用数据存储池，开启压缩，适用于JuiceFS不适用的场景（如普通数据库）
      - name: ceph-application-compressed
        spec:
          failureDomain: osd  # 设置存储池的故障域
          replicated: # 设置为副本池
            size: 3  # 副本数
          deviceClass: nvme  # 设置后端osd类型
          parameters:
            compression_mode: aggressive # 设置除了指定不可压缩的数据以外都进行压缩
            pg_num: "512" # 设置初始的PG数量
            pgp_num: "512" # 设置初始的PGP数量，需要和pg_num一致
        storageClass:
          enabled: true
          name: ceph-application-compressed  # 存储类名称
          isDefault: true  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小
          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

#      # 基于Ceph RBD提供服务的低性能应用数据存储池，开启压缩，使用HDD存储，适用于只需要冷存储的场景（如Elasticsearch的冷数据节点）
#      - name: ceph-application-low-performance-compressed
#        spec:
#          failureDomain: osd  # 设置存储池的故障域
#          replicated: # 设置为副本池
#            size: 3  # 副本数
#          deviceClass: hdd  # 设置后端osd类型
#          parameters:
#            compression_mode: aggressive # 设置除了指定不可压缩的数据以外都进行压缩
#            pg_num: "128" # 设置初始的PG数量
#            pgp_num: "128" # 设置初始的PGP数量，需要和pg_num一致
#        storageClass:
#          enabled: true
#          name: ceph-application-low-performance-compressed  # 存储类名称
#          isDefault: false  # 是否设为默认存储类
#          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
#          allowVolumeExpansion: true  # 支持动态调整PVC大小
#          parameters:
#            # 默认设置不用动
#            imageFormat: "2"
#            imageFeatures: layering
#            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
#            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
#            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
#            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
#            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
#            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
#            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      # 基于Ceph RBD提供服务的高性能应用数据存储池，不开启压缩，适用于需要高性能的场景（如Elasticsearch的热数据节点）
      - name: ceph-application-high-performance
        spec:
          failureDomain: osd  # 设置存储池的故障域
          replicated: # 设置为副本池
            size: 3  # 副本数
          deviceClass: nvme  # 设置后端osd类型
          parameters:
            pg_num: "512" # 设置初始的PG数量
            pgp_num: "512" # 设置初始的PGP数量，需要和pg_num一致
        storageClass:
          enabled: true
          name: ceph-application-high-performance  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小
          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐
      
      # 基于JuiceFS提供服务的高性能应用数据存储池，不开启压缩，适用于需要高性能的场景
      # ⚠️不适用于有大量随机写（会使JuiceFS产生大量碎片）的应用，碎片会导致JuiceFS占用巨量空间，尤其是数据库持续高负载时极其容易将存储撑爆
      - name: juicefs-application-high-performance
        spec:
          failureDomain: osd  # 设置存储池的故障域
          replicated: # 设置为副本池
            size: 3  # 副本数
          deviceClass: nvme  # 设置后端osd类型
          parameters:
            pg_num: "32" # 设置初始的PG数量
            pgp_num: "32" # 设置初始的PGP数量，需要和pg_num一致
        # 由于是JuiceFS使用的存储池，所以不需要创建Ceph CSI提供的K8S StorageClass，而是使用JuiceFS CSI提供的K8S StorageClass
        storageClass:
          enabled: false  # 是否启用对应的K8S StorageClass，可以仅创建Ceph存储池但不创建StorageClass

      # 基于Ceph RBD提供服务的高性能虚拟机数据存储池，不开启压缩，适用于虚拟机系统盘之类的高性能场景
      - name: ceph-vm-high-performance
        spec:
          failureDomain: osd  # 设置存储池的故障域
          replicated: # 设置为副本池
            size: 3  # 副本数
          deviceClass: nvme  # 设置后端osd类型
          parameters:
            pg_num: "512" # 设置初始的PG数量
            pgp_num: "512" # 设置初始的PGP数量，需要和pg_num一致
        storageClass:
          enabled: true
          name: ceph-vm-high-performance  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小
          parameters:
            # 设置虚拟机专用的优化参数，解决Windows虚拟机出现IO Error的问题
            # https://bugzilla.redhat.com/show_bug.cgi?id=2109455
            # https://docs.ceph.com/en/latest/man/8/rbd/#kernel-rbd-krbd-options
            mounter: rbd
            mapOptions: "krbd:rxbounce"
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      - name: juicefs-media-erasure-coded
        spec:
          failureDomain: osd  # 设置存储池的故障域
          erasureCoded: # 设置为EC池
            dataChunks: 6   # EC数据块数
            codingChunks: 2 # EC编码数据块数
          deviceClass: hdd  # 设置后端osd类型
          parameters:
            pg_num: "128" # 设置初始的PG数量
            pgp_num: "128" # 设置初始的PGP数量，需要和pg_num一致
        storageClass:
          enabled: false
      
      - name: juicefs-compressed-files-erasure-coded
        spec:
          failureDomain: osd  # 设置存储池的故障域
          erasureCoded: # 设置为EC池
            dataChunks: 6   # EC数据块数
            codingChunks: 2 # EC编码数据块数
          deviceClass: hdd  # 设置后端osd类型
          parameters:
            compression_mode: aggressive # 设置除了指定不可压缩的数据以外都进行压缩
            pg_num: "128" # 设置初始的PG数量
            pgp_num: "128" # 设置初始的PGP数量，需要和pg_num一致
        storageClass:
          enabled: false

    # CephFS存储池和对应的K8S StorageClass设置
    # Rook会默认设置一个，在不需要使用CephFS的情况下，将该设置项显式设为空列表以避免该存储池创建
    cephFileSystems: [ ]

    # Ceph RGW存储池和对应的K8S StorageClass设置
    # Rook会默认设置一个，在不需要使用RGW的情况下，将该设置项显式设为空列表以避免该存储池创建
    cephObjectStores: [ ]
