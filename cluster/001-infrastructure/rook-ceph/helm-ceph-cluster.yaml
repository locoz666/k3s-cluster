---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m
  timeout: 30m
  install:
    remediation:
      retries: 3
  chart:
    spec:
      # renovate: registryUrl=https://charts.rook.io/release
      chart: rook-ceph-cluster
      version: v1.12.2
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  values:
    operatorNamespace: rook-ceph

    # 设置默认的存储池副本数和最小副本数，以免无副本存储池无法工作
    # 不让mon报存储池无副本的warning
    configOverride: |
      [global]
      mon_allow_pool_delete = true
      osd_pool_default_size = 1
      osd_pool_default_min_size = 1
      bdev_enable_discard = true
      bdev_async_discard = true
      [mon]
      mon_warn_on_pool_no_redundancy = false
      mon_data_avail_warn = 10
      [osd]
      bluefs_buffered_io = false
      osd_scrub_max_interval = 2592000
      osd_scrub_load_threshold = 10
      osd_max_scrubs = 10
      osd_scrub_during_recovery = false
      osd_max_backfills = 64
      osd_recovery_op_priority = 120
      osd_backfill_retry_interval = 0
      osd_recovery_sleep = 0
      osd_recovery_max_active = 512
      osd_recovery_max_single_start = 512

    toolbox:
      enabled: true

    cephClusterSpec:
      # 是否在pg状态非完全健康的情况下也允许升级OSD
      continueUpgradeAfterChecksEvenIfNotHealthy: true

      mon:
        # 允许在同一个节点上运行多个mon
        allowMultiplePerNode: true

      mgr:
        # 允许在同一个节点上运行多个mgr
        allowMultiplePerNode: true
        modules:
          # PG自动缩放器
          - name: pg_autoscaler
            enabled: false
          # 硬盘健康状态预测
          - name: diskprediction_local
            enabled: true
          # Rook Orchestrator扩展
          - name: rook
            enabled: true

      dashboard:
        # 手动指定rook ceph的dashboard为http模式，避免因为自签名证书导致无法正常通过ingress访问
        ssl: false

      placement:
        all:
          # 将所有ceph服务设置为必须在有ceph角色的节点上运行
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/ceph
                      operator: Exists

      # 资源限制
      resources:
        prepareosd:
          limits:
            cpu: "500m"
            memory: "8192Mi"
        mgr:
          limits:
            memory: "16384Mi"
          requests:
            memory: "4096Mi"
        # osd在进行recovery操作时可以吃到8G左右的内存，limit设置需要尽量高于这个值，否则可能会出现osd被系统杀死的情况
        osd:
          limits:
            memory: "8192Mi"
          requests:
            memory: "1Gi"
        # mds的元数据缓存默认有限制只会使用4GB内存，但实际在大量小文件写入时会出现占用到几十GB且不清理的情况，做个限制以防万一
        mds:
          limits:
            memory: "4096Mi"
          requests:
            memory: "4096Mi"

      storage:
        # 关闭自动在所有节点和所有磁盘上寻找空白盘并添加为OSD的操作，改为手动配置方式添加
        useAllNodes: false
        useAllDevices: false
        nodes:
          - name: "home-server-titan"
            devices:
              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD41FWR"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD463ZF"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD6XBFD"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD71JSA"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0JMTX"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0NB7Y"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0NC8L"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SWDC_WUH722020BL5204_8LHPJ7WN"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SWDC_WUH722020BL5204_8LHPZ97R"
                config:
                  deviceClass: "hdd"
              - name: "/dev/disk/by-id/scsi-SWDC_WUH722020BL5204_8LHPZ1KR"
                config:
                  deviceClass: "hdd"
          - name: "home-server-sonic"
            devices:
              - name: "/dev/disk/by-id/nvme-INTEL_SSDPE2KX040T8_BTLJ848209YE4P0DGN"  # Intel P4510 4T U.2
                config:
                  deviceClass: "nvme"
                  osdsPerDevice: "4"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS且降低读写延迟（https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments）
              - name: "/dev/disk/by-id/nvme-INTEL_SSDPE2KX040T8_PHLJ8293003U4P0DGN"  # Intel P4510 4T U.2
                config:
                  deviceClass: "nvme"
                  osdsPerDevice: "4"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS且降低读写延迟（https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments）
              - name: "/dev/disk/by-id/nvme-INTEL_SSDPF2KX076T1_BTAX253201AQ7P6DGN"  # Intel P5520 7.68T U.2
                config:
                  deviceClass: "nvme"
                  osdsPerDevice: "4"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS且降低读写延迟（https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments）
              - name: "/dev/disk/by-id/nvme-INTEL_SSDPF2KX076T1_BTAX2532023H7P6DGN"  # Intel P5520 7.68T U.2
                config:
                  deviceClass: "nvme"
                  osdsPerDevice: "4"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS且降低读写延迟（https://tracker.ceph.com/projects/ceph/wiki/Tuning_for_All_Flash_Deployments）


      #        nodes:
      #          - name: "home-server-nut"
      #            devices:
      #              - name: "/dev/disk/by-id/ata-Samsung_SSD_870_EVO_1TB_S6PVNF0R604091N"
      #                config:
      #                  deviceClass: "ssd"
      #                  osdsPerDevice: "2"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS，但会略微降低读写延迟（ns级）
      #
      #              - name: "/dev/disk/by-id/scsi-SATA_Samsung_SSD_870_S6PVNX0RC01477A"
      #                config:
      #                  deviceClass: "ssd"
      #                  osdsPerDevice: "2"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS，但会略微降低读写延迟（ns级）

      healthCheck:
        livenessProbe:
          osd:
            probe:
              initialDelaySeconds: 240

    ingress:
      dashboard:
        ingressClassName: "traefik"
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: "kube-system-rfc1918@kubernetescrd"
        host:
          name: "ceph.${SECRET_DOMAIN}"
        tls:
          - secretName: "${SECRET_DOMAIN/./-}-tls"

    cephBlockPools:
      # 普通应用，开启压缩，使用nvme ssd
      - name: block-application
        spec:
          failureDomain: osd
          replicated:
            size: 3  # 副本数
            requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: nvme  # 设置后端osd类型
          parameters:
            compression_mode: aggressive # 设置除了指定不可压缩的数据以外都进行压缩
        storageClass:
          enabled: true
          name: ceph-block-application  # 存储类名称
          isDefault: true  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      # 需要极致性能的应用，不压缩，使用nvme ssd
      - name: block-application-performance
        spec:
          failureDomain: osd
          replicated:
            size: 3  # 副本数
            requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: nvme  # 设置后端osd类型
        storageClass:
          enabled: true
          name: ceph-block-application-performance  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      # 写入速度无所谓，但需要高速读取且确保存储可靠性的应用，使用nvme ssd作为主副本提高读取性能，使用hdd做热备副本
      - name: block-application-hybrid
        spec:
          failureDomain: osd
          replicated:
            size: 3  # 副本数
            requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
            hybridStorage: # 设置为混合存储池，主副本为SSD，其余副本为HDD
              primaryDeviceClass: nvme
              secondaryDeviceClass: hdd
        storageClass:
          enabled: true
          name: ceph-block-application-hybrid  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

    cephFileSystems: [ ]
    cephObjectStores: [ ]
