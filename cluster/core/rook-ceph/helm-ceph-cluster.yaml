---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://charts.rook.io/release
      chart: rook-ceph-cluster
      version: v1.7.5
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  values:
    operatorNamespace: rook-ceph

    # 设置默认的存储池副本数和最小副本数，以免无副本存储池无法工作
    # 不让mon报存储池无副本的warning
    configOverride: |
      [global]
      mon_allow_pool_delete = true
      osd_pool_default_size = 1
      osd_pool_default_min_size = 1
      [mon]
      mon_warn_on_pool_no_redundancy = false
      [osd]
      osd_max_scrubs = 5
      osd_scrub_during_recovery = true
      osd_max_backfills = 1000
      osd_backfill_scan_min = 300
      osd_backfill_scan_max = 500
      osd_backfill_retry_interval = 0
      osd_recovery_sleep_hdd = 0
      osd_recovery_max_active_hdd = 1000
      osd_recovery_max_single_start = 1000
      osd_recovery_max_chunk = 268435456

    toolbox:
      enabled: true

    cephClusterSpec:
      # 在pg状态非完全健康的情况下也允许升级OSD
      continueUpgradeAfterChecksEvenIfNotHealthy: true

      mon:
        # 允许在同一个节点上运行多个mon
        allowMultiplePerNode: true

      placement:
        all:
          # 将所有ceph服务设置为必须在有ceph角色的节点上运行
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/ceph
                      operator: Exists
          # 容忍gateway污点
          tolerations:
            - key: gateway
              operator: Exists
        mon:
          # 将mon设置为必须在有ceph-mon角色的节点上运行
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/ceph-mon
                      operator: Exists
        mgr:
          # 将mgr设置为必须在有ceph-mgr角色的节点上运行
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/ceph-mgr
                      operator: Exists

      # 资源限制
      resources:
        mgr:
          limits:
            memory: "16384Mi"
          requests:
            memory: "4096Mi"
        # osd在进行recovery操作时可以吃到8G左右的内存，limit设置需要尽量高于这个值，否则可能会出现osd被系统杀死的情况
        osd:
          limits:
            memory: "8192Mi"
        # mds的元数据缓存默认有限制只会使用4GB内存，但实际在大量小文件写入时会出现占用到几十GB且不清理的情况，做个限制以防万一
        mds:
          limits:
            memory: "4096Mi"
          requests:
            memory: "4096Mi"

      storage:
        # 关闭自动在所有节点和所有磁盘上寻找空白盘并添加为OSD的操作，改为手动配置方式添加
        useAllNodes: false
        useAllDevices: false
        nodes:
          - name: "home-server-nut"
            devices:
              - name: "/dev/disk/by-id/ata-Samsung_SSD_870_EVO_1TB_S6PVNF0R604091N"
                config:
                  deviceClass: "ssd"
                  osdsPerDevice: "2"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS，但会略微降低读写延迟（ns级）

              - name: "/dev/disk/by-id/scsi-SATA_SanDisk_SDSSDA12_163646475411"
                config:
                  deviceClass: "ssd"
                  osdsPerDevice: "2"

              - name: "/dev/disk/by-id/scsi-SATA_SanDisk_SDSSDA24_173130448503"
                config:
                  deviceClass: "ssd"
                  osdsPerDevice: "2"

              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD71JSA"
                config:
                  deviceClass: "hdd"

              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD41FWR"
                config:
                  deviceClass: "hdd"

              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD6XBFD"
                config:
                  deviceClass: "hdd"

              - name: "/dev/disk/by-id/ata-ST6000NM0115-1YZ110_ZAD463ZF"
                config:
                  deviceClass: "hdd"

          - name: "home-edge-gateway"
            devices:
              - name: "sda"
                config:
                  deviceClass: "hdd"

              - name: "sdb"
                config:
                  deviceClass: "hdd"

      healthCheck:
        livenessProbe:
          osd:
            probe:
              initialDelaySeconds: 240

    ingress:
      dashboard:
        annotations:
          kubernetes.io/ingress.class: "traefik"
          kubernetes.io/tls-acme: "true"
        host:
          name: "ceph.${SECRET_DOMAIN}"
        tls:
          - hosts:
              - "ceph.${SECRET_DOMAIN}"
            secretName: ceph-dashboard-tls

    cephBlockPools:
      - name: block-hdd-r2
        spec:
          failureDomain: osd
          replicated:
            size: 2  # 副本数
            requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: hdd  # 设置后端osd类型
        storageClass:
          enabled: true
          name: ceph-block-hdd-r2  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      - name: block-ssd-r1
        spec:
          failureDomain: osd
          replicated:
            size: 1  # 副本数
            requireSafeReplicaSize: false  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: ssd  # 设置后端osd类型
        storageClass:
          enabled: true
          name: ceph-block-ssd-r1  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      - name: block-ssd-r2
        spec:
          failureDomain: osd
          replicated:
            size: 2  # 副本数
            requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: ssd  # 设置后端osd类型
        storageClass:
          enabled: true
          name: ceph-block-ssd-r2  # 存储类名称
          isDefault: true  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

    cephFileSystems:
      - name: fs-hdd-r2
        spec:
          metadataPool:
            failureDomain: osd
            replicated:
              size: 2  # 副本数，多副本确保元数据可靠性
            deviceClass: ssd  # 设置后端osd类型，使用ssd加速metadata读写和响应速度
          dataPools:
            - failureDomain: osd
              replicated:
                size: 2  # 副本数
                requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
              deviceClass: hdd  # 设置后端osd类型
          metadataServer:
            # 默认设置不用动
            activeCount: 1
            activeStandby: true
        storageClass:
          enabled: true
          name: ceph-fs-hdd-r2  # 存储类名称
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          parameters:
            # 默认设置不用动
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

      - name: fs-hdd-ec
        spec:
          metadataPool:
            failureDomain: osd
            replicated:
              size: 3  # 副本数，多副本确保元数据可靠性
            deviceClass: ssd  # 设置后端osd类型，使用ssd加速metadata读写和响应速度
          dataPools:
            - failureDomain: osd
              erasureCoded:
                dataChunks: 4
                codingChunks: 2
              deviceClass: hdd  # 设置后端osd类型
          metadataServer:
            # 默认设置不用动
            activeCount: 1
            activeStandby: true
        storageClass:
          enabled: true
          name: ceph-fs-hdd-ec  # 存储类名称
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          parameters:
            # 默认设置不用动
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

    cephObjectStores: [ ]
