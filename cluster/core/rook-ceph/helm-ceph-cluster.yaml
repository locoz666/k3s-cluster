---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://charts.rook.io/release
      chart: rook-ceph-cluster
      version: v1.7.2
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  values:
    operatorNamespace: rook-ceph

    # 设置默认的存储池副本数和最小副本数，以免无副本存储池无法工作
    # 不让mon报存储池无副本的warning
    configOverride: |
      [global]
      mon_allow_pool_delete = true
      osd_pool_default_size = 1
      osd_pool_default_min_size = 1
      [mon]
      mon_warn_on_pool_no_redundancy = false

    toolbox:
      enabled: true

    cephClusterSpec:
      mon:
        # 允许在同一个节点上运行多个mon
        allowMultiplePerNode: true

      # 将所有ceph集群服务设置为必须在有ceph角色的节点上运行
      placement:
        all:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/ceph
                      operator: Exists

      storage:
        # 关闭自动在所有节点和所有磁盘上寻找空白盘并添加为OSD的操作，改为手动配置方式添加
        useAllNodes: false
        useAllDevices: false
        nodes:
          - name: "home-server-nut"
            devices:
              - name: "/dev/disk/by-id/ata-Samsung_SSD_870_EVO_1TB_S6PVNF0R604091N"
                config:
                  deviceClass: "ssd"
                  osdsPerDevice: "2"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS，但会略微降低读写延迟（ns级）

              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD71JSA"
                config:
                  deviceClass: "hdd"

              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD41FWR"
                config:
                  deviceClass: "hdd"

              - name: "/dev/disk/by-id/ata-ST8000NM000A-2KE101_WRD0NC8L"
                config:
                  deviceClass: "hdd"

              - name: "/dev/disk/by-id/ata-ST8000NM000A-2KE101_WRD0JMTX"
                config:
                  deviceClass: "hdd"

    ingress:
      dashboard:
        annotations:
          kubernetes.io/ingress.class: "traefik"
          kubernetes.io/tls-acme: "true"
        host:
          name: "ceph.${SECRET_DOMAIN}"
        tls:
          - hosts:
              - "ceph.${SECRET_DOMAIN}"
            secretName: ceph-dashboard-tls

    cephBlockPools:
      - name: block-hdd-r1
        spec:
          failureDomain: osd
          replicated:
            size: 1  # 副本数
            requireSafeReplicaSize: false  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: hdd  # 设置后端osd类型
        storageClass:
          enabled: true
          name: ceph-block-hdd-r1  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      - name: block-hdd-r2
        spec:
          failureDomain: osd
          replicated:
            size: 2  # 副本数
            requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: hdd  # 设置后端osd类型
        storageClass:
          enabled: true
          name: ceph-block-hdd-r2  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      - name: block-ssd-r1
        spec:
          failureDomain: osd
          replicated:
            size: 1  # 副本数
            requireSafeReplicaSize: false  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: ssd  # 设置后端osd类型
        storageClass:
          enabled: true
          name: ceph-block-ssd-r1  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

    cephFileSystems:
      - name: ceph-filesystem-l1
        spec:
          metadataPool:
            failureDomain: osd
            replicated:
              size: 3
          dataPools:
            - failureDomain: osd
              replicated:
                size: 1
                requireSafeReplicaSize: false
          metadataServer:
            activeCount: 1
            activeStandby: true
        storageClass:
          enabled: true
          name: ceph-fs-l1
          reclaimPolicy: Delete
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

      - name: ceph-filesystem-l2
        spec:
          metadataPool:
            failureDomain: osd
            replicated:
              size: 3
          dataPools:
            - failureDomain: osd
              replicated:
                size: 2
          metadataServer:
            activeCount: 1
            activeStandby: true
        storageClass:
          enabled: true
          name: ceph-fs-l2
          reclaimPolicy: Delete
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4

    cephObjectStores: [ ]
