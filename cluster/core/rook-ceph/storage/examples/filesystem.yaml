---
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: ceph-fs-l1 # fs名，对应存储池名
  namespace: rook-ceph
spec:
  # The metadata pool spec. Must use replication.
  metadataPool:
    failureDomain: osd  # 故障域，可以设置为host或osd等
    replicated:
      size: 3  # 副本数量，设为1既为无副本
      requireSafeReplicaSize: true  # 如果要设置无副本的就改成false
    parameters:
      # Inline compression mode for the data pool
      # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression
      compression_mode:
        none
        # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
      # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
      #target_size_ratio: ".5"
  # The list of data pool specs. Can use replication or erasure coding.
  dataPools:
    - failureDomain: osd  # 故障域，可以设置为host或osd等
      replicated:
        size: 1  # 副本数量，设为1既为无副本
        # Disallow setting pool with replica 1, this could lead to data loss without recovery.
        # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
        requireSafeReplicaSize: false  # 如果要设置无副本的就改成false
      parameters:
        # Inline compression mode for the data pool
        # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression
        compression_mode:
          none
          # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
        # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
        #target_size_ratio: ".5"
  # Whether to preserve filesystem after CephFilesystem CRD deletion
  preserveFilesystemOnDelete: true
  # The metadata service (mds) configuration
  metadataServer:
    # The number of active MDS instances
    activeCount: 1
    # Whether each active MDS instance will have an active standby with a warm metadata cache for faster failover.
    # If false, standbys will be available, but will not have a warm cache.
    activeStandby: false
    # The affinity rules to apply to the mds deployment
    # placement:
    #   #  nodeAffinity:
    #   #    requiredDuringSchedulingIgnoredDuringExecution:
    #   #      nodeSelectorTerms:
    #   #      - matchExpressions:
    #   #        - key: role
    #   #          operator: In
    #   #          values:
    #   #          - mds-node
    #   #  topologySpreadConstraints:
    #   #  tolerations:
    #   #  - key: mds-node
    #   #    operator: Exists
    #   #  podAffinity:
    #   podAntiAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       - labelSelector:
    #           matchExpressions:
    #             - key: app
    #               operator: In
    #               values:
    #                 - rook-ceph-mds
    #         # topologyKey: kubernetes.io/hostname will place MDS across different hosts
    #         topologyKey: kubernetes.io/hostname
    #     preferredDuringSchedulingIgnoredDuringExecution:
    #       - weight: 100
    #         podAffinityTerm:
    #           labelSelector:
    #             matchExpressions:
    #               - key: app
    #                 operator: In
    #                 values:
    #                   - rook-ceph-mds
    #           # topologyKey: */zone can be used to spread MDS across different AZ
    #           # Use <topologyKey: failure-domain.beta.kubernetes.io/zone> in k8s cluster if your cluster is v1.16 or lower
    #           # Use <topologyKey: topology.kubernetes.io/zone>  in k8s cluster is v1.17 or upper
    #           topologyKey: topology.kubernetes.io/zone
    # A key/value list of annotations
    annotations:
    #  key: value
    # A key/value list of labels
    labels:
    #  key: value
    resources:
    # The requests and limits set here, allow the filesystem MDS Pod(s) to use half of one CPU core and 1 gigabyte of memory
    #  limits:
    #    cpu: "500m"
    #    memory: "1024Mi"
    #  requests:
    #    cpu: "500m"
    #    memory: "1024Mi"
    # priorityClassName: my-priority-class
  mirroring:
    enabled: false

---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-fs-l1
provisioner: rook-ceph.cephfs.csi.ceph.com # driver:namespace:operator
parameters:
  # clusterID is the namespace where operator is deployed.
  clusterID: rook-ceph # namespace:cluster

  # CephFS filesystem name into which the volume shall be created
  fsName: ceph-fs-l1

  # Ceph pool into which the volume shall be created
  # Required for provisionVolume: "true"
  pool: ceph-fs-l1-data0

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster

  # (optional) The driver can use either ceph-fuse (fuse) or ceph kernel client (kernel)
  # If omitted, default volume mounter will be used - this is determined by probing for ceph-fuse
  # or by setting the default mounter explicitly via --volumemounter command-line argument.
  # mounter: kernel
reclaimPolicy: Retain  # 回收策略，设为Retain以手动控制
allowVolumeExpansion: true
mountOptions:
  # uncomment the following line for debugging
  #- debug
