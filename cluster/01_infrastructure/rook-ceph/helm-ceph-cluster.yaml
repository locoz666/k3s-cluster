---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m
  timeout: 30m
  install:
    remediation:
      retries: 3
  chart:
    spec:
      # renovate: registryUrl=https://charts.rook.io/release
      chart: rook-ceph-cluster
      version: v1.11.4
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  values:
    operatorNamespace: rook-ceph

    # 设置默认的存储池副本数和最小副本数，以免无副本存储池无法工作
    # 不让mon报存储池无副本的warning
    configOverride: |
      [global]
      mon_allow_pool_delete = true
      osd_pool_default_size = 1
      osd_pool_default_min_size = 1
      bdev_enable_discard = true
      bdev_async_discard = true
      [mon]
      mon_warn_on_pool_no_redundancy = false
      [osd]
      bluefs_buffered_io = false
      osd_scrub_begin_hour = 0
      osd_scrub_end_hour = 24
      osd_scrub_load_threshold = 10
      osd_scrub_chunk_min = 32
      osd_scrub_chunk_max = 128
      osd_max_scrubs = 5
      osd_scrub_during_recovery = true
      osd_max_backfills = 5
      osd_backfill_scan_min = 32
      osd_backfill_scan_max = 128
      osd_backfill_retry_interval = 0
      osd_recovery_sleep = 0
      osd_recovery_max_active = 5
      osd_recovery_max_single_start = 5
      osd_recovery_max_chunk = 268435456

    toolbox:
      enabled: true

    cephClusterSpec:
      # 是否在pg状态非完全健康的情况下也允许升级OSD
      continueUpgradeAfterChecksEvenIfNotHealthy: true

      mon:
        # 允许在同一个节点上运行多个mon
        allowMultiplePerNode: true

      mgr:
        # 允许在同一个节点上运行多个mgr
        allowMultiplePerNode: true

      dashboard:
        # 手动指定rook ceph的dashboard为http模式，避免因为自签名证书导致无法正常通过ingress访问
        ssl: false

      placement:
        all:
          # 将所有ceph服务设置为必须在有ceph角色的节点上运行
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: node-role.kubernetes.io/ceph
                      operator: Exists

      # 资源限制
      resources:
        prepareosd:
          limits:
            cpu: "500m"
            memory: "8192Mi"
        mgr:
          limits:
            memory: "16384Mi"
          requests:
            memory: "4096Mi"
        # osd在进行recovery操作时可以吃到8G左右的内存，limit设置需要尽量高于这个值，否则可能会出现osd被系统杀死的情况
        osd:
          limits:
            memory: "8192Mi"
          requests:
            cpu: "100m"
            memory: "4Gi"
        # mds的元数据缓存默认有限制只会使用4GB内存，但实际在大量小文件写入时会出现占用到几十GB且不清理的情况，做个限制以防万一
        mds:
          limits:
            memory: "4096Mi"
          requests:
            memory: "4096Mi"

      storage:
        # 关闭自动在所有节点和所有磁盘上寻找空白盘并添加为OSD的操作，改为手动配置方式添加
        useAllNodes: false
        useAllDevices: false
      #        nodes:
      #          - name: "home-server-nut"
      #            devices:
      #              - name: "/dev/disk/by-id/ata-Samsung_SSD_870_EVO_1TB_S6PVNF0R604091N"
      #                config:
      #                  deviceClass: "ssd"
      #                  osdsPerDevice: "2"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS，但会略微降低读写延迟（ns级）
      #
      #              - name: "/dev/disk/by-id/scsi-SATA_Samsung_SSD_870_S6PVNX0RC01477A"
      #                config:
      #                  deviceClass: "ssd"
      #                  osdsPerDevice: "2"  # 在单块盘上创建多个OSD以最大化利用SSD的性能，可以让OSD获得更多CPU资源进行并发读写，能提升IOPS，但会略微降低读写延迟（ns级）
      #
      #              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD71JSA"
      #                config:
      #                  deviceClass: "hdd"
      #
      #              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD41FWR"
      #                config:
      #                  deviceClass: "hdd"
      #
      #              - name: "/dev/disk/by-id/scsi-SATA_ST6000NM0115-1YZ_ZAD6XBFD"
      #                config:
      #                  deviceClass: "hdd"
      #
      #              - name: "/dev/disk/by-id/ata-ST6000NM0115-1YZ110_ZAD463ZF"
      #                config:
      #                  deviceClass: "hdd"
      #
      #              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0NC8L"
      #                config:
      #                  deviceClass: "hdd"
      #
      #              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0NB7Y"
      #                config:
      #                  deviceClass: "hdd"
      #
      #              - name: "/dev/disk/by-id/scsi-SATA_ST8000NM000A-2KE_WRD0JMTX"
      #                config:
      #                  deviceClass: "hdd"

      healthCheck:
        livenessProbe:
          osd:
            probe:
              initialDelaySeconds: 240

    ingress:
      dashboard:
        ingressClassName: "traefik"
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: "kube-system-rfc1918@kubernetescrd"
        host:
          name: "ceph.${SECRET_DOMAIN}"
        tls:
          - secretName: "${SECRET_DOMAIN/./-}-tls"

    cephBlockPools:
      # 普通应用，开启压缩，使用nvme ssd作为主副本提高读取性能，使用hdd做热备副本
      - name: block-application
        spec:
          failureDomain: osd
          replicated:
            size: 3  # 副本数
            requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
            hybridStorage: # 设置为混合存储池，主副本为SSD，其余副本为HDD
              primaryDeviceClass: nvme
              secondaryDeviceClass: hdd
          parameters:
            compression_mode: aggressive # 设置除了指定不可压缩的数据以外都进行压缩
        storageClass:
          enabled: true
          name: ceph-block-application  # 存储类名称
          isDefault: true  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

      # 需要极致性能的应用，不压缩，使用nvme ssd
      - name: block-application-performance
        spec:
          failureDomain: osd
          replicated:
            size: 3  # 副本数
            requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
          deviceClass: nvme  # 设置后端osd类型
        storageClass:
          enabled: true
          name: ceph-block-application-performance  # 存储类名称
          isDefault: false  # 是否设为默认存储类
          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
          allowVolumeExpansion: true  # 支持动态调整PVC大小

          parameters:
            # 默认设置不用动
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4  # 可以设置为xfs，但官方不推荐

    cephFileSystems: [ ]
    #    cephFileSystems:
    #      - name: fs-application
    #        spec:
    #          metadataPool:
    #            failureDomain: osd
    #            replicated:
    #              size: 3  # 副本数，多副本确保元数据可靠性
    #              requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #            deviceClass: ssd  # 设置后端osd类型，使用ssd加速metadata读写和响应速度
    #          dataPools:
    #            - failureDomain: osd
    #              replicated:
    #                size: 3  # 副本数
    #                requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #              deviceClass: ssd  # 设置后端osd类型
    #          metadataServer:
    #            # 默认设置不用动
    #            activeCount: 1
    #            activeStandby: true
    #        storageClass:
    #          enabled: true
    #          name: ceph-fs-application  # 存储类名称
    #          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
    #          parameters:
    #            # 默认设置不用动
    #            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    #            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/fstype: ext4
    #
    #      - name: fs-media
    #        spec:
    #          metadataPool:
    #            failureDomain: osd
    #            replicated:
    #              size: 3  # 副本数，多副本确保元数据可靠性
    #              requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #            deviceClass: ssd  # 设置后端osd类型，使用ssd加速metadata读写和响应速度
    #          dataPools:
    #            - failureDomain: osd
    #              replicated:
    #                size: 3  # 副本数
    #                requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #              deviceClass: hdd  # 设置后端osd类型
    #          metadataServer:
    #            # 默认设置不用动
    #            activeCount: 1
    #            activeStandby: true
    #        storageClass:
    #          enabled: true
    #          name: ceph-fs-media  # 存储类名称
    #          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
    #          parameters:
    #            # 默认设置不用动
    #            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    #            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/fstype: ext4
    #
    #      - name: fs-media-unsafe
    #        spec:
    #          metadataPool:
    #            failureDomain: osd
    #            replicated:
    #              size: 3  # 副本数，多副本确保元数据可靠性
    #              requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #            deviceClass: ssd  # 设置后端osd类型，使用ssd加速metadata读写和响应速度
    #          dataPools:
    #            - failureDomain: osd
    #              replicated:
    #                size: 1  # 副本数
    #                requireSafeReplicaSize: false  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #              deviceClass: hdd  # 设置后端osd类型
    #          metadataServer:
    #            # 默认设置不用动
    #            activeCount: 1
    #            activeStandby: true
    #        storageClass:
    #          enabled: true
    #          name: ceph-fs-media-unsafe  # 存储类名称
    #          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
    #          parameters:
    #            # 默认设置不用动
    #            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    #            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/fstype: ext4
    #
    #      - name: fs-temp
    #        spec:
    #          metadataPool:
    #            failureDomain: osd
    #            replicated:
    #              size: 1  # 副本数，多副本确保元数据可靠性
    #              requireSafeReplicaSize: false  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #            deviceClass: ssd  # 设置后端osd类型，使用ssd加速metadata读写和响应速度
    #          dataPools:
    #            - failureDomain: osd
    #              replicated:
    #                size: 1  # 副本数
    #                requireSafeReplicaSize: false  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #              deviceClass: hdd  # 设置后端osd类型
    #          metadataServer:
    #            # 默认设置不用动
    #            activeCount: 1
    #            activeStandby: true
    #        storageClass:
    #          enabled: true
    #          name: ceph-fs-temp  # 存储类名称
    #          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
    #          parameters:
    #            # 默认设置不用动
    #            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    #            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/fstype: ext4
    #
    #      - name: fs-compressed-files
    #        spec:
    #          metadataPool:
    #            failureDomain: osd
    #            replicated:
    #              size: 3  # 副本数，多副本确保元数据可靠性
    #              requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #            deviceClass: ssd  # 设置后端osd类型，使用ssd加速metadata读写和响应速度
    #          dataPools:
    #            - failureDomain: osd
    #              replicated:
    #                size: 3  # 副本数
    #                requireSafeReplicaSize: true  # 安全选项，副本数为1时需要设为false，否则会无法创建
    #              deviceClass: hdd  # 设置后端osd类型
    #              parameters:
    #                compression_mode: aggressive # 设置除了指定不可压缩的数据以外都进行压缩
    #          metadataServer:
    #            # 默认设置不用动
    #            activeCount: 1
    #            activeStandby: true
    #        storageClass:
    #          enabled: true
    #          name: ceph-fs-compressed-files  # 存储类名称
    #          reclaimPolicy: Delete  # PVC被删除后自动删除池中的内容
    #          parameters:
    #            # 默认设置不用动
    #            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    #            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    #            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
    #            csi.storage.k8s.io/fstype: ext4

    cephObjectStores: [ ]
