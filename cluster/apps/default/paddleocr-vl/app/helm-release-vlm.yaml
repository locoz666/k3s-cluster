---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: paddleocr-vlm-server
spec:
  interval: 5m
  timeout: 30m
  install:
    remediation:
      retries: 3
  chart:
    spec:
      # renovate: registryUrl=https://bjw-s.github.io/helm-charts
      chart: app-template
      version: 4.3.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s
        namespace: flux-system
  values:
    controllers:
      main:
        type: deployment
        replicas: 1
        strategy: Recreate
        pod:
          securityContext:
            runAsNonRoot: false
            runAsUser: 0
          affinity:
            nodeAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  preference:
                    matchExpressions:
                      - key: kubernetes.io/hostname
                        operator: In
                        values:
                          - home-server-power
                - weight: 50
                  preference:
                    matchExpressions:
                      - key: kubernetes.io/hostname
                        operator: In
                        values:
                          - home-server-titan
                - weight: 25
                  preference:
                    matchExpressions:
                      - key: kubernetes.io/hostname
                        operator: In
                        values:
                          - home-server-sonic
                - weight: 10
                  preference:
                    matchExpressions:
                      - key: kubernetes.io/hostname
                        operator: In
                        values:
                          - home-server-nut
        containers:
          main:
            image:
              repository: ccr-2vdh3abv-pub.cnc.bj.baidubce.com/paddlepaddle/paddleocr-genai-vllm-server
              tag: latest-nvidia-gpu
              pullPolicy: IfNotPresent
            command:
              - /bin/bash
              - -lc
            args:
              - |
                exec paddleocr genai_server \
                  --model_name PaddleOCR-VL-1.5-0.9B \
                  --host 0.0.0.0 \
                  --port 8080 \
                  --backend vllm \
                  --backend_config /home/paddleocr/vlm_server_config.yaml
            env:
              TZ: Asia/Shanghai
              HOME: /home/paddleocr
              PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK: "True"
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8080
                  initialDelaySeconds: 30
                  periodSeconds: 30
                  timeoutSeconds: 5
                  failureThreshold: 10
              readiness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8080
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 30
              startup:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health
                    port: 8080
                  initialDelaySeconds: 300
                  periodSeconds: 10
                  timeoutSeconds: 5
                  failureThreshold: 360
            resources:
              requests:
                cpu: 10m
                memory: 16Gi
                nvidia.com/gpu: 1
              limits:
                cpu: 16
                memory: 32Gi
                nvidia.com/gpu: 1

    service:
      main:
        controller: main
        type: ClusterIP
        ports:
          http:
            port: 8080
            targetPort: 8080
            protocol: HTTP

    persistence:
      backend-config:
        type: configMap
        name: "{{ .Release.Name }}"
        advancedMounts:
          main:
            main:
              - path: /home/paddleocr/vlm_server_config.yaml
                subPath: vlm_server_config.yaml
                readOnly: true
      model-cache:
        existingClaim: paddleocr-vl-model-cache
        advancedMounts:
          main:
            main:
              - path: /home/paddleocr/.paddlex
      shm:
        type: emptyDir
        medium: Memory
        sizeLimit: 8Gi
        advancedMounts:
          main:
            main:
              - path: /dev/shm
                readOnly: false

    configMaps:
      config:
        data:
          vlm_server_config.yaml: |
            tokenizer-mode: slow
